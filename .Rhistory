train1 <- train.xform[1, pred.vars]
train1
#
# class(prob1)
# class(train1)
test.xform <- rbind(train1 , test.xform)
pred.vars <- c("cvtd_timestamp", "num_window", "roll_belt", "pitch_belt", "yaw_belt", "magnet_dumbbell_y", "magnet_dumbbell_z")
prob1 <- test.xform[1, pred.vars]
prob1
train1 <- train.xform[1, pred.vars]
train1
#
# class(prob1)
# class(train1)
test.xform <- rbind(train.xform[1, ] , test.xform)[-1, ]
pred.vars <- c("cvtd_timestamp", "num_window", "roll_belt", "pitch_belt", "yaw_belt", "magnet_dumbbell_y", "magnet_dumbbell_z")
prob1 <- test.xform[1, pred.vars]
prob1
train1 <- train.xform[1, pred.vars]
train1
#
# class(prob1)
# class(train1)
test.xform <- rbind(train.xform[1, -57] , test.xform)[-1, -57]
pred.vars <- c("cvtd_timestamp", "num_window", "roll_belt", "pitch_belt", "yaw_belt", "magnet_dumbbell_y", "magnet_dumbbell_z")
prob1 <- test.xform[1, pred.vars]
prob1
train1 <- train.xform[1, pred.vars]
train1
#
# class(prob1)
# class(train1)
test.xform <- rbind(train.xform[1, -57] , test.xform[, -57])[-1, ]
predict(model.rf, xtest)
pred.vars <- c("cvtd_timestamp", "num_window", "roll_belt", "pitch_belt", "yaw_belt", "magnet_dumbbell_y", "magnet_dumbbell_z")
prob1 <- test.xform[1, pred.vars]
prob1
train1 <- train.xform[1, pred.vars]
train1
#
# class(prob1)
# class(train1)
test.xform <- rbind(train.xform[1, -57] , test.xform[, -57])[-1, ]
predict(model.rf, test.xform)
rf.feature.selection(train.xform[, c("cvtd_timestamp", "num_window", "roll_belt", "pitch_belt", "yaw_belt", "magnet_dumbbell_y", "magnet_dumbbell_z", "classe")], 8, feature.size = 5)
rf.feature.selection(train.xform[, c("cvtd_timestamp", "num_window", "roll_belt", "pitch_belt", "yaw_belt", "magnet_dumbbell_y", "magnet_dumbbell_z", "classe")], 8, feature.size = 1)
model.rf<-randomForest(classe ~ cvtd_timestamp + num_window + roll_belt + yaw_belt,
data=train.xform,importance=TRUE,ntree=100)
test.xform <- rbind(train.xform[1, -57] , test.xform[, -57])[-1, ]
predict(model.rf, test.xform)
# B A B A A E D B A A B C B A E E A B B B
# B A B A A E D B A A B C B A E E A B B B
model.rf<-randomForest(classe ~ cvtd_timestamp + num_window,
importance=TRUE,ntree=100)
model.rf<-randomForest(classe ~ cvtd_timestamp + num_window,
data=train.xform,importance=TRUE,ntree=100)
test.xform <- rbind(train.xform[1, -57] , test.xform[, -57])[-1, ]
predict(model.rf, test.xform)
# B A B A A E D B A A B C B A E E A B B B
# B A B A A E D B A A B C B A E E A B B B
# B A B A A E D B A A B C B A E E A B B B
modFit <- train(class ~ cvtd_timestamp + num_window, data=train.xform, method = "rf", prox = TRUE)
modFit <- train(class ~ cvtd_timestamp + num_window,
data = train.xform, method = "rf", prox = TRUE)
modFit <- train(class ~ cvtd_timestamp + num_window,
data = train.xform, method = "rf", prox = TRUE, na.action = na.pass)
modFit <- train(class ~ cvtd_timestamp + num_window,
data = train.xform, method = "rf", na.action = na.pass)
modFit <- train(class ~ cvtd_timestamp + num_window,
data = train.xform, method = "rf", na.action = na.pass)
modFit <- train(class ~ cvtd_timestamp + num_window,
data = train.xform, method = "rf", na.action = na.pass)
modFit <- train(class ~ cvtd_timestamp + num_window,
data = train.xform, method = "rf", na.action = na.pass)
modFit <- train(class ~ cvtd_timestamp + num_window,
data = train.xform, method = "rf", na.action = na.pass)
modFit <- train(class ~ cvtd_timestamp + num_window,
data = train.xform, method = "rf", na.action = na.pass)
modFit <- train(class ~ cvtd_timestamp + num_window,
data = train.xform, method = "rf", na.action = na.pass)
modFit <- train(class ~ cvtd_timestamp + num_window,
data = train.xform, method = "rf", na.action = na.pass)
modFit <- train(class ~ cvtd_timestamp + num_window,
data = train.xform, method = "rf", na.action = na.pass)
modFit <- train(class ~ cvtd_timestamp + num_window,
data = train.xform, method = "rf", na.action = na.pass)
modFit <- train(classe ~ cvtd_timestamp + num_window,
data = train.xform, method = "rf", prox = TRUE)
# ensure the results are repeatable
set.seed(7)
# load the library
library(mlbench)
library(caret)
# load the data
data(PimaIndiansDiabetes)
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(PimaIndiansDiabetes[,1:8], PimaIndiansDiabetes[,9], sizes=c(1:8), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
library(mlbench)
library(caret)
# Load Dataset
data(Sonar)
dataset <- Sonar
x <- dataset[,1:60]
y <- dataset[,61]
# Create model with default paramters
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7
metric <- "Accuracy"
set.seed(seed)
mtry <- sqrt(ncol(x))
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(Class~., data=dataset, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_default)
# Random Search
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
set.seed(seed)
mtry <- sqrt(ncol(x))
rf_random <- train(Class~., data=dataset, method="rf", metric=metric, tuneLength=15, trControl=control)
print(rf_random)
plot(rf_random)
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
set.seed(2018 - 7 - 8)
tunegrid <- expand.grid(.mtry=c(1:15))
rf_gridsearch <- train(Class~., data=dataset, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
set.seed(seed)
tunegrid <- expand.grid(.mtry=c(1:15))
rf_gridsearch <- train(Class~., data=dataset, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)
# train model
control <- trainControl(method="repeatedcv", number=10, repeats=3)
tunegrid <- expand.grid(.mtry=c(1:15), .ntree=c(1000, 1500, 2000, 2500))
set.seed(seed)
custom <- train(Class~., data=dataset, method=customRF,
metric=metric, tuneGrid=tunegrid, trControl=control)
customRF <- list(type = "Classification", library = "randomForest", loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"),
class = rep("numeric", 2),
label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {
}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {
predict(modelFit, newdata)
}
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {
predict(modelFit, newdata, type = "prob")
}
customRF$sort <- function(x) {
x[order(x[,1]),]
}
customRF$levels <- function(x) {
x$classes
}
# train model
control <- trainControl(method="repeatedcv", number=10, repeats=3)
tunegrid <- expand.grid(.mtry=c(1:15), .ntree=c(1000, 1500, 2000, 2500))
set.seed(seed)
custom <- train(Class~., data=dataset, method=customRF,
metric=metric, tuneGrid=tunegrid, trControl=control)
summary(custom)
plot(custom)
summary(custom$results)
custom$results
attach(custom$results)
custom$results[order(Accuracy)]
class(custom$results)
custom$results[order(Accuracy)]
attach(custom$results)
custom$results[order(Accuracy), ]
attach(custom$results)
custom$results[order(-Accuracy), ]
knitr::opts_chunk$set(echo = TRUE)
training <- read.csv("pml-training.csv",
na.strings=c('#DIV/0!', '', 'NA'),
stringsAsFactors = F)
knitr::opts_chunk$set(echo = TRUE)
training <- read.csv("pml-training.csv",
na.strings=c('#DIV/0!', '', 'NA'),
stringsAsFactors = F)
testing  <- read.csv("pml-testing.csv",
na.strings=c('#DIV/0!', '', 'NA'),
stringsAsFactors = F)
apply(training, 2, class)
training <- read.csv("pml-training.csv",
na.strings=c('#DIV/0!', '', 'NA'))
testing  <- read.csv("pml-testing.csv",
na.strings=c('#DIV/0!', '', 'NA'),
stringsAsFactors = F)
apply(training, 2, class)
?read.csv
default.stringsAsFactors()
training <- read.csv("pml-training.csv",
na.strings=c('#DIV/0!', '', 'NA'))
testing  <- read.csv("pml-testing.csv",
na.strings=c('#DIV/0!', '', 'NA'),
stringsAsFactors = F)
head(training)
training <- read.csv("pml-training.csv", na.strings=c('#DIV/0!', '', 'NA'))
testing  <- read.csv("pml-testing.csv", na.strings=c('#DIV/0!', '', 'NA'))
head(training)
head(training)
str(training)
default.stringsAsFactors()
str(training)
aggr(training)
sum(is.na(training))/(dim(training)[1]*dim(training)[2])
# Missing Values fraction by column / variable
missCol <- apply(training, 2, function(x) sum(is.na(x)/length(x)))
# Distribution of Missing Variables
hist(missCol, main = "Missing Data by Variable")
# table(missCol)
missIndCol <- which(missCol > 0.9)
length(missIndCol)  #Number of predictors > 90% missing
# Remove Missing Variables from training and test sets
train.xform.temp <- training[,-missIndCol]
test.xform.temp  <- testing[, -missIndCol]
# Remove X = row count variable, and raw time stamps
# train.xform  <- train.xform.temp[,-c(1,3,4)]
# test.xform   <- test.xform.temp[,-c(1,3,4)]
aggr(train.xform)
View(train.xform.temp)
# Remove Missing Variables from training and test sets
train.xform.temp <- training[,-missIndCol]
test.xform.temp  <- testing[, -missIndCol]
# Remove X = row count variable, and raw time stamps
train.xform  <- train.xform.temp[,-c(1,3,4)]
test.xform   <- test.xform.temp[,-c(1,3,4)]
aggr(train.xform)
# Remove Missing Variables from training and test sets
train.xform.temp <- training[,-missIndCol]
test.xform.temp  <- testing[, -missIndCol]
# Remove X = row count variable, and raw time stamps
train.xform  <- train.xform.temp[,-c(1,3,4)]
test.xform   <- test.xform.temp[,-c(1,3,4)]
aggr(train.xform)
var.predict<-paste(names(train.xform)[-57],collapse="+")
rf.form <- as.formula(paste(names(train.xform)[57], var.predict, sep = " ~ "))
rf.form
modfit <- randomForest(rf.form,data=train.xform,importance=TRUE,
ntree=100, na.action = na.pass)
modfit
modfit <- randomForest(rf.form,data=train.xform,importance=TRUE,
ntree=100, na.action = na.pass)
modfit
varImpPlot(modfit)
imp.table <- data.frame(modfit$importance)
ord <- order(imp.table$MeanDecreaseAccuracy, decreasing = TRUE)
names(train.xform)[ord]
modfit <- randomForest(classe ~ user_name + cvtd_timestamp + num_window +
roll_belt + pitch_belt + yaw_belt + total_accel_belt + gyros_belt_x +
gyros_belt_y + gyros_belt_z + accel_belt_x + accel_belt_y +
accel_belt_z + magnet_belt_x + magnet_belt_y + magnet_belt_z +
roll_arm + pitch_arm + yaw_arm + total_accel_arm + gyros_arm_x +
gyros_arm_y + gyros_arm_z + accel_arm_x + accel_arm_y + accel_arm_z +
magnet_arm_x + magnet_arm_y + magnet_arm_z + roll_dumbbell +
pitch_dumbbell + yaw_dumbbell + total_accel_dumbbell + gyros_dumbbell_x +
gyros_dumbbell_y + gyros_dumbbell_z + accel_dumbbell_x +
accel_dumbbell_y + accel_dumbbell_z + magnet_dumbbell_x +
magnet_dumbbell_y + magnet_dumbbell_z + roll_forearm + pitch_forearm +
yaw_forearm + total_accel_forearm + gyros_forearm_x + gyros_forearm_y +
gyros_forearm_z + accel_forearm_x + accel_forearm_y + accel_forearm_z +
magnet_forearm_x + magnet_forearm_y + magnet_forearm_z,
data=train.xform, importance=TRUE, ntree=100, na.action = na.pass)
modfit
pred.vars <- names(train.xform)[-57]
var.predict<-paste(pred.vars, collapse="+")
rf.form <- as.formula(paste(names(train.xform)[57], var.predict, sep = " ~ "))
rf.form
modfit <- randomForest(rf.form,data=train.xform,importance=TRUE,
ntree=100, na.action = na.pass)
modfit
varImpPlot(modfit)
imp.table <- data.frame(modfit$importance)
ord <- order(imp.table$MeanDecreaseAccuracy, decreasing = TRUE)
names(train.xform)[ord]
imp.table <- data.frame(modfit$importance)
ord <- order(imp.table$MeanDecreaseAccuracy, decreasing = TRUE)
pord <- order(imp.table$MeanDecreaseAccuracy, decreasing = FALSE)
pred.vars[ord]
pred.vars <- pred.vars[-pord[1]]
var.predict<-paste(pred.vars, collapse="+")
rf.form <- as.formula(paste(names(train.xform)[57], var.predict, sep = " ~ "))
modfit <- randomForest(rf.form, data=train.xform,
importance=TRUE, ntree=100, na.action = na.pass)
modfit
imp.table <- data.frame(modfit$importance)
ord <- order(imp.table$MeanDecreaseAccuracy, decreasing = TRUE)
pred.vars[ord]
# Create model with default paramters
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 2018 - 7 - 8
metric <- "Accuracy"
set.seed(seed)
mtry <- sqrt(ncol(train.xform) - 1)
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(Class~., data=train.xform, method="rf",
metric=metric, tuneGrid=tunegrid, trControl=control)
# Create model with default paramters
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 2018 - 7 - 8
metric <- "Accuracy"
set.seed(seed)
mtry <- sqrt(ncol(train.xform) - 1)
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(classe ~ ., data=train.xform, method="rf",
metric=metric, tuneGrid=tunegrid, trControl=control)
start.time <- Sys.time()
# Create model with default paramters
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 2018 - 7 - 8
metric <- "Accuracy"
set.seed(seed)
mtry <- sqrt(ncol(train.xform) - 1)
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(classe ~ ., data=train.xform, method="rf",
metric=metric, tuneGrid=tunegrid, trControl=control)
start.time <- Sys.time()
# Create model with default paramters
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 2018 - 7 - 8
metric <- "Accuracy"
set.seed(seed)
mtry <- sqrt(ncol(train.xform) - 1)
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(classe ~ ., data=train.xform, method="rf",
metric=metric, tuneGrid=tunegrid, trControl=control)
View(rf_default)
rf_default
paste("End in", Sys.time() - start.time, sep = " ")
rf_default
?train
start.time <- Sys.time()
# Random Search
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
set.seed(seed)
mtry <- sqrt(ncol(train.xform) - 1)
rf_random <- train(classe ~ ., data=train.xform, method="rf",
metric=metric, tuneLength=15, trControl=control)
setwd("~/Desktop/r-workspace/Practical Machine Learning/ass")
source('~/.active-rstudio-document', echo=TRUE)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
# Training data
training <- read.csv("pml-training.csv", na.strings=c('#DIV/0!', '', 'NA'))
# Testing data
testing  <- read.csv("pml-testing.csv", na.strings=c('#DIV/0!', '', 'NA'))
str(training)
summary(training)
dim(training)
View(testing)
# Remove Missing Variables from training and test sets
new.training <- training[,-missIndCol]
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
# Training data
training <- read.csv("pml-training.csv", na.strings=c('#DIV/0!', '', 'NA'))
# Testing data
testing  <- read.csv("pml-testing.csv", na.strings=c('#DIV/0!', '', 'NA'))
dim(training)
aggr(training)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(tidyr)
library(dplyr)
library(ggplot2)
library(caret)
library(glmnet)
library(ranger)
library(VIM)
# Training data
training <- read.csv("pml-training.csv", na.strings=c('#DIV/0!', '', 'NA'))
# Testing data
testing  <- read.csv("pml-testing.csv", na.strings=c('#DIV/0!', '', 'NA'))
dim(training)
aggr(training)
sum(is.na(training))/(dim(training)[1]*dim(training)[2])
# Missing Values fraction by column / variable
missCol <- apply(training, 2, function(x) sum(is.na(x)/length(x)))
# Distribution of Missing Variables
hist(missCol, main = "Missing Data by Variable")
# table(missCol)
missIndCol <- which(missCol > 0.9)
length(missIndCol)  #Number of predictors > 90% missing
# Remove Missing Variables from training and test sets
new.training <- training[,-missIndCol]
new.testing  <- testing[, -missIndCol]
# Remove X = row count variable, and raw time stamps
new.training  <- new.training[,-c(1,3,4)]
new.testing   <- new.testing[,-c(1,3,4)]
dim(new.training)
aggr(train.xform)
aggr(new.training)
modfit <- randomForest(classe ~ ., data = new.training,
importance=TRUE, ntree=100, na.action = na.pass)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(tidyr)
library(dplyr)
library(ggplot2)
library(caret)
library(glmnet)
library(ranger)
library(VIM)
library(randomForest)
modfit <- randomForest(classe ~ ., data = new.training,
importance=TRUE, ntree=100, na.action = na.pass)
modfit
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(tidyr)
library(dplyr)
library(ggplot2)
library(caret)
library(glmnet)
library(ranger)
library(VIM)
library(randomForest)
set.seed(2018 - 7 - 8)
# Training data
training <- read.csv("pml-training.csv", na.strings=c('#DIV/0!', '', 'NA'))
# Testing data
testing  <- read.csv("pml-testing.csv", na.strings=c('#DIV/0!', '', 'NA'))
dim(training)
aggr(training)
sum(is.na(training))/(dim(training)[1]*dim(training)[2])
# Missing Values fraction by column / variable
missCol <- apply(training, 2, function(x) sum(is.na(x)/length(x)))
# Distribution of Missing Variables
hist(missCol, main = "Missing Data by Variable")
# table(missCol)
missIndCol <- which(missCol > 0.9)
length(missIndCol)  #Number of predictors > 90% missing
# Remove Missing Variables from training and test sets
new.training <- training[,-missIndCol]
new.testing  <- testing[, -missIndCol]
# Remove X = row count variable, and raw time stamps
new.training  <- new.training[,-c(1,3,4)]
new.testing   <- new.testing[,-c(1,3,4)]
dim(new.training)
aggr(new.training)
modfit <- randomForest(classe ~ ., data = new.training,
importance=TRUE, ntree=100, na.action = na.pass)
modfit
modfit <- randomForest(classe ~ ., data = new.training,
importance=TRUE, ntree=100, na.action = na.pass)
modfit
varImpPlot(modfit)
imp.table <- data.frame(modfit$importance)
ord <- order(imp.table$MeanDecreaseAccuracy, decreasing = TRUE)
names(train.xform)[ord]
imp.table <- data.frame(modfit$importance)
ord <- order(imp.table$MeanDecreaseAccuracy, decreasing = TRUE)
names(new.training)[ord]
imp.table <- data.frame(modfit$importance)
ord <- order(imp.table$MeanDecreaseAccuracy, decreasing = TRUE)
names(new.training)[ord][1:20]
imp.table <- data.frame(modfit$importance)
ord <- order(imp.table$MeanDecreaseAccuracy, decreasing = TRUE)
names(new.training)[ord][1:30]
imp.table <- data.frame(modfit$importance)
ord <- order(imp.table$MeanDecreaseAccuracy, decreasing = TRUE)
predictors <- names(new.training)[ord][1:30]
predictors
rf.formula <- as.formula(paste("classe",
paste(predictors, collapse="+"),
sep = " ~ "))
rf.formula
rf.formula <- as.formula(paste("classe",
paste(predictors, collapse="+"),
sep = " ~ "))
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "Accuracy"
mtry <- 20
tunegrid <- expand.grid(.mtry=mtry)
start.time <- Sys.time()
rfmodel <- train(classe ~ ., data=new.training, method="rf",
metric=metric, tuneGrid=tunegrid, trControl=control)
duration
rf.formula <- as.formula(paste("classe",
paste(predictors, collapse="+"),
sep = " ~ "))
control <- trainControl(method="repeatedcv", number=10, repeats=3)
metric <- "Accuracy"
mtry <- 20
tunegrid <- expand.grid(.mtry=mtry)
start.time <- Sys.time()
rfmodel <- train(rf.formula, data=new.training, method="rf",
metric=metric, tuneGrid=tunegrid, trControl=control)
duration <- Sys.time() - start.time
rfmodel
duration
predict(rfmodel, newdata = new.testing)
View(rfmodel)
rfmodel$results
rfmodel$results[2]
round(rfmodel$results[2], 4)
round(rfmodel$results[2] * 100, 2)
duration
class(duration)
duration
pwd()
